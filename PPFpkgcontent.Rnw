\section*{Abstract}
The \pkg{PPforest} package (short for Projection pursuit classification random forest) implements this new ensemble learning method introduced in \cite{dasilvappforest}. 
In the PPforest, each split is based on a linear combination of randomly chosen variables. The linear combination is computed by optimizing a projection pursuit index, to get a projection of the variables that best separates the classes. The PPforest uses the \pkg{PPtree} algorithm, which fits a single tree to the data. Utilizing linear combinations of variables to separate classes takes the correlation between variables into account, and can outperform the basic forest when separations between groups occur on combinations of variables. Two projection pursuit indexes, LDA and PDA, are used for PPforest.
To improve the speed performance of PPforest package, the main functions were implemented in Rcpp, that implies the original PPtree algorithm was translated and other function which required to be fast.
PPforest package utilizes a number of R packages some of them included in ``suggests'' not to load them all at package start-up.
In this user manual, the key functions included in the package are presented and some visualizations using PPforest diagnostics.

% \section{Projection pursuit classification forest}
% In PPforest, projection pursuit classification trees  are used as the individual model to be combined in the forest. The original algorithm is in \pkg{PPtreeViz} package,  we translate the original tree algorithm into \pkg{Rcpp} to improve the speed performance to run the forest.

% One important characteristic of PPtree is that treats the data always as a two-class system,  when the classes are more than two the algorithm uses a two step  projection pursuits optimization in every node split.
% Let  $(X_i,y_i)$ the data set, $X_i$ is a  p-dimensional vector of explanatory variables and  $y_i\in {1,2,\ldots G}$ represents class information with $i=1,\ldots n$.
% 
% In the first step optimize a projection pursuit index to find an optimal one-dimension projection $\alpha^*$ for separating all classes in the current data. With the projected data redefine the problem in a two class problem by comparing means, and assign a new label $g_1$ or $g_2$ to each observation, a new variable $y_i^*$ is created.  The new groups $g_1$ and $g_2$ can contain more than one original classes. Next step is to find an optimal one-dimensional projection $\alpha^{**}$, using $(X_i,y_i^*)$ to separate the two class problem $g_1$ and $g_2$. The best separation of $g_1$ and $g_2$ is determined in this step and the decision rule is defined for the current node, if $\sum_{i=1}^p \alpha_i M1< c$ then assign $g_1$ to the left node else assign $g_2$ to the right node, where $M1$ is the mean of $g_1$.
% For each groups we can repeat all the previous steps until $g_1$ and $g_2$ have only one class from the original classes. Base on this process to grow the tree, the depth of PPtree is at most the number of classes because one class is assigned only to one final node.
% 
% Trees from \pkg{PPtree} algorithm are simple, they use the association between variables to find separation. If a linear boundary exists, \pkg{PPtree} produces a tree without misclassification.
% 
% Projection pursuit random forest algorithm description
% 
% \begin{enumerate}
% \item Let N the number of cases in the training set $\Theta=(X,Y)$, $B$ bootstrap samples from the training set are taking (samples of size N with replacement).
% 
% \item For each bootstrap sample a \verb PPtree  is grown to the largest extent possible $h(x, {\Theta_k})$. No pruning. This tree is grown using step 3 modification.
% 
% \item Let M the number of input variables, a number of $m<<M$ variables are selected at random at each node and the best split based on a linear combination of these randomly chosen variables. The linear combination is computed by optimizing a projection pursuit index, to get a projection of the variables that best separates the classes.
% 
% \item  Predict the classes of each case not included in the bootstrap sample and compute oob error.
% 
% \item  Based on majority vote predict the class for new data.
% \end{enumerate}
\section{Package description and illustrative examples}
In this section, a detailed description of \pkg{PPforest} package along with examples is presented. To get detailed information about PPforest method check \cite{dasilvappforest}
\pkg{PPforest} package implements a classification random forest using projection pursuit classification trees.

\subsection{Functions and data}
Table \ref{tabfun} shows the complete list of functions included in the package along with a brief description. 
%A summary of the main functions in \pkg{PPforest} is presented.
% 
% \begin{itemize}
% \item Bagged projection pursuit trees are implemented.
% \item Projected data and boundaries for each node partition are available.
% \item Predictions for test data and error calculation.
% \item Projection pursuit random forest for classification problems
% \item Importance variable measures.
% \item Data structure to plot generalized ternary plots using vote matrix information.
% \end{itemize}
 \begin{center}
 \begin{table}[!h]
  \begin{tabular}{ l | p{12cm} } \hline
\textbf{Function} &\textbf{Description} \\ \hline
 {\tt baggtree}&For each bootstrap sample grow a projection pursuit tree (PPtree object)\\ \\ \hline
 {\tt node\_data} &Data structure with the  projected and boundary by node and class\\ \\ \hline
 {\tt PPclassify2}&Predict class for a test dataset and calculate prediction error\\ \\ \hline
 {\tt PPforest} &Runs a Projection pursuit random forest\\ \\ \hline
 {\tt permute\_importance}&Obtain the permuted importance variable measure\\ \\ \hline
 {\tt PPtree\_split}&Projection pursuit classification tree with random variable selection in each split\\ \\ \hline
 {\tt ppf\_avg\_pptree\_imp}& Computes a global importance measure for a PPforest object, average\\ \\ \hline
 {\tt ppf\_global\_imp}& Computes a global importance measure for a PPforest object\\ \\ \hline
 {\tt print.PPforest}& Print PPforest object\\ \\ \hline
 {\tt predict.PPforest}&Predict class for the test set and calculate prediction error\\ \\ \hline
 {\tt ternary\_str}&Data structure with the  projected and boundary by node and class\\ \\ \hline
 {\tt tree\_pred}&Obtain predicted class for new data using PPforest t.\\ \\ 
   \hline
  \end{tabular}
  \caption{Summary of functions implemented in \pkg{PPforest}\label{tabfun}}
  \end{table}
\end{center}
The developed version for the package is available on GitHub and will be soon on CRAN. To install \pkg{PPforest} you should run the following code:
<<echo = TRUE, message=FALSE, warning=FALSE,eval=FALSE>>=
library(devtools)
install_github("natydasilva/PPforest")
library(PPforest)
@
Australian crab data set will be used as example across this manual. This data contains measurements on rock crabs of the genus Leptograpsus. There are 200 observations from two species (blue and orange) and for each specie (50 in each one) there are 50 males and 50 females. Class variable has 4 classes with the combinations of specie and sex (BlueMale, BlueFemale, OrangeMale and OrangeFemale). The data were collected on site at Fremantle, Western Australia. For each specimen, five measurements were made, using vernier calipers.

\begin{itemize}
\item FL the size of the frontal lobe length, in mm
\item RW rear width, in mm
\item CL length of mid line of the carapace, in mm
\item CW maximum width of carapace, in mm
\item BD depth of the body; for females, measured after displacement of the abdomen, in mm
\end{itemize}

 A scatterplot matrix using \pkg{GGally} \citep{ggally} package is used to visualize crab data set presented in  Figure \ref{scatercrab}.  In this figure there is shows a strong, positive and linear association between the different variables. Also look like the classes can be separated by linear combinations of variables. This example is a case where PPforest outperforms the classic Random Forest because the linear projections in each node partitions thakes into account the correlation between variables.

<< echo=FALSE, message=FALSE, warning=FALSE>>=
require(PPforest)
require(RColorBrewer)
require(GGally)
require(gridExtra)
require(PPtreeViz)
require(plotly)
require(dplyr)
library(ggplot2)
@
\begin{figure}[!h]
<<fig.show='hold', fig.width = 5, fig.height = 5, echo=FALSE>>=

a <- GGally::ggpairs(PPforest::crab,
    columns = 2:6,
    ggplot2::aes(colour = Type, alpha = .1),
    lower = list(continuous = 'points'),
    axisLabels = 'none',
    upper = list(continuous='blank')
     , legend = NULL)

a
@
\caption{Scatter plot matrix for carb data \label{scatercrab}}
\end{figure}
\pkg{PPforest} includes some data set that were used to test its predictive performance. The data sets included are summarized in Table \ref{datappf}.
\begin{center}
\begin{table}[!h]
\begin{tabular}{ l | p{12cm} } \hline
\textbf{Data} & \textbf{Description}\\ \hline
{\tt crab} & Measurements on rock crabs of the genus Leptograpsus \\ \\ \hline
{\tt fishcatch} & Measurements on fishes caught form Finland\\ \\ \hline
{\tt glass} & Measurements on 6 different types of class\\ \\ \hline
{\tt image} & Instances from 7 outoor images\\ \\ \hline
{\tt leukemia} & Gene expression data set in two types of acute leukemias\\ \\ \hline
{\tt lymphoma} & Gene expression in the three most prevalent adult lymphoid malignancies\\ \\ \hline
{\tt NCI60} & Gene expression data among the 60 cell lines\\ \\ \hline
{\tt parkinson} & Data set containing 195 observations from 2 parkinson types \\ \\ \hline
{\tt wine} & Data set containing observations from 3 wine grown cultivares in Italy\\  \\ \hline
  \end{tabular}
  \caption{Summary of functions implemented in \pkg{PPforest}\label{datappf}}
  \end{table}
\end{center}

\subsection{{\tt PPForest}}

The main function of the package is called {\tt PPforest} which implements a projection pursuit random forest. Table \ref{ppforestfun} presents the {\tt PPforest} function arguments and their respective description.
  \begin{center}
 \begin{table}[!h]
  \begin{tabular}{ l | p{12cm} } \hline
  \textbf{Arguments} & \textbf{Description}\\ \hline
{\tt data}	& Data frame with the complete data set\\ \hline
{\tt class}	& A character with the name of the class variable.\\ \hline
{\tt std} & if TRUE standardize the data set, needed to compute global importance measure\\ \hline
{\tt size.tr} & is the size proportion of the training if we want to split the data in\\ &training and test\\ \hline
 {\tt m}&is the number of bootstrap replicates, this corresponds with the number of trees to\\ 
 &grow. To ensure that each observation is predicted a few times we have to select\\ 
 &this number no too small. m = 500 is by default\\ \hline
{\tt PPmethod} & is the projection pursuit index to optimize in each classification tree. The options are\\
&LDA and PDA, linear discriminant and penalized linear discriminant. By default it is LDA\\ \hline
{\tt size.p} & proportion of variables randomly sampled in each split\\ \hline
 {\tt lambda}& penalty parameter in PDA index and is between 0 to 1. If lambda = 0, no penalty\\
 &parameter is added and the PDA index is the same as LDA index. If lambda = 1\\
 &all variables are treated as uncorrelated. The default value is lambda = 0.1\\ \hline
 {\tt parallel} &
logical condition, if it is TRUE then parallelize the function\\ \hline
 {\tt cores} &number of cores used in the parallelization\\ \hline
  \end{tabular}
  \caption{Summary of available arguments of function {\tt PPforest} \label{ppforestfun}}
  \end{table}
  \end{center}
  
{\tt PPforest} function runs a projection pursuit random forest. The arguments are a data frame with the data information, class with the name of the class variable argument.  {\tt size.tr} to specify the proportion of observations using in the training. Using this function we have the option to split the data in training and test using {\tt size.tr} directly. {\tt size.tr} is the proportion of data used in the training and the test proportion will be 1- {\tt size.tr}.
The number of trees in the forest is specified using the argument {\tt m}. The argument {\tt size.p} is the sample proportion of the variables used in each node split, {\tt PPmethod} is the projection pursuit index to be optimized,  two options LDA and PDA are available.
The algorithm can be parallelized specifing {\tt parallel =TRUE} and the number of used cores can be incuded in {\tt core} core argument.



The following code is to run {\tt PPforest} using crab data, in this case all the observations available are used to run the forest ({\tt size.tr = 1}), the number of trees is 200 ( {\tt m= 200})  and the proportion of variables used in each node partitions is .5 ({\tt size.p = .5} in this case 2 variables). The selected projection index is `LDA' ({\tt PPmethod =}) and the forest is parallelized ({\tt parallel = TRUE}).
<<tidy=TRUE>>=
set.seed(146)
pprf.crab <- PPforest::PPforest(data = crab, class = "Type", size.tr = 1, 
                                m = 200, size.p =  .5,  PPmethod = 'LDA',  
                                parallel =TRUE, cores = 2)
@

{\tt PPforest} print a summary result from the model with the confusion matrix information and the oob-error rate in a similar way \pkg{randomForest} packages does to make them comparable. Based on confusion matrix, we can observe that the biggest error is for BlueMale class. Most of the wrong classified values are between BlueFemale and BlueMale.

<<tidy=TRUE>>=
pprf.crab
@
This function returns the predicted values of the training data, training error, test error and predicted test values. Also there is the information about out of bag error for the forest and also for each tree in the forest. Bootstrap samples, output of all the trees in the forest from , proximity matrix and vote matrix, number of trees grown in the forest, number of predictor variables selected to use for splitting at each node. Confusion matrix of the prediction (based on OOb data), the training data and test data and vote matrix are also returned.

<<echo=TRUE,tidy=TRUE>>=
 str(pprf.crab, max.level=1, list.len=5)
@

  \begin{center}
 \begin{table}[!h]
  \begin{tabular}{ l | p{12cm} } \hline
  \textbf{Value} & \textbf{Description}\\ \hline
{\tt prediction.training}	& predicted values for training data se\\  \hline
{\tt training.error}	&error of the training data set.\\  \hline
{\tt prediction.test} & predicted values for the test data set if testap = TRUE(default\\ \hline
{\tt oob.error.forest} & out of bag error in the forest\\  \hline
 {\tt oob.error.tree}&out of bag error for each tree in the forest\\ \hline
{\tt boot.samp} & information of bootrap samples.\\ \hline
{\tt output.trees} & output from a {\tt trees\_pp} for each bootrap sample\\  \hline
 {\tt proximity}& Proximity matrix, if two cases are classified in the same terminal node then the proximity matrix is increased by one in PPforest there are one terminal node per class\\  \hline

 {\tt votes} & a matrix with one row for each input data point and one column for each class, giving the fraction of (OOB) votes from the PPforest\\  \hline
 {\tt n.tree} &number of trees grown in PPforest\\  \hline
 {\tt n.var} &number of predictor variables selected to use for spliting at each node\\  \hline
 {\tt type} &classification\\  \hline
 {\tt confusion} &confusion matrix of the prediction (based on OOb data)\\  \hline
 {\tt call} & the original call to PPforest. \\  \hline
 {\tt train}& is the training data based on size.tr sample proportion\\  \hline
 {\tt test}& is the test data based on $1-${\tt size.tr} sample proportion\\  \hline
  \end{tabular}
  \caption{Summary of available values of function {\tt PPforest} \label{ppforestout}}
  \end{table}
  \end{center}

\newpage 

\subsection{{\tt PPtree\_split}}
{\tt PPtree\_split} function implements a projection pursuit classification tree with random variable selection in each split, based on the original PPtreeViz algorithm. This function returns a PPtreeclass object.
A summary of available arguments and its descriprion is presented in Table \ref{ppsplitfun}
 \begin{center}
 \begin{table}[!h]
  \begin{tabular}{ l | p{12cm} } \hline
  \textbf{Arguments} & \textbf{Description}\\ \hline
{\tt form} & 	A character with the name of the class variable\\ \hline
{ \tt data} &	Data frame with the complete data set \\ \hline
{ \tt PPmethod} &	index to use for projection pursuit: `LDA', `PDA' \\ \hline
{\tt size.p} & 	proportion of variables randomly sampled in each split, default is 1, returns a PPtree pbject\\ \hline
{\tt lambda} &	penalty parameter in PDA index and is between 0 to 1 . If lambda = 0, no penalty parameter is added and the PDA index is the same as LDA index. If lambda = 1 all variables are treated as uncorrelated. The default value is lambda = 0.1\\ \hline
{\tt...}&	arguments to be passed to methods\\ \hline
  \end{tabular}
  \caption{Summary of available arguments of function {\tt PPtree\_split} \label{ppsplitfun}}
  \end{table}
  \end{center}
To run one tree with random variables selection an example using crab data is presented bellow. In this case LDA method is used and the number of varibles used in each node partition is also 2 ({\tt size.p =0.5}).
<<echo=TRUE,tidy=TRUE>>=
Tree.crab <- PPforest::PPtree_split("Type~.", data = crab,
                                    PPmethod = "LDA", size.p = 0.5)
@

<<echo=TRUE,tidy=TRUE>>=
str(Tree.crab, max.level = TRUE)
@
  
 \begin{center}
 \begin{table}[!h]
  \begin{tabular}{ l | p{12cm} } \hline
  \textbf{Values} & \textbf{Description}\\ \hline
{\tt Tree.Struct} & 	Tree structure of projection pursuit classification tree\\ \hline
{ \tt projbest.node} & 1-dim optimal projections of each split node \\ \hline
{ \tt splitCutoff.node} &	cutoff values of each split node\\ \hline
{\tt origclass} & original class\\ \hline
{\tt origdata} &	original data\\ \hline
  \end{tabular}
  \caption{Summary of putput values of function {\tt PPtree\_split} \label{ppsplitfunout}}
  \end{table}
  \end{center}  
  
  

% baggtree this function grow a PPtreeclass using PPtree\_split for each bootstrap sample.
% This function returns a data frame with the results from PPtree\_split for each bootsrap samples.
% m is the number of trees here is a small example.
% <<>>=
%  crab.trees <- baggtree(data = crab, class = "Type",
%   m =  10, PPmethod = 'LDA', size.p = 0.6 )
%  str(crab.trees, max.level = 1)
% 
%  #selecting first PPtree object,
%  crab.trees[[1]][[1]]
% @

% 
% The PPforest algorithm calculates variable importance in two ways: (1) permuted importance using accuracy,  and (2) importance based on projection coefficients on standardized variables.
% 
% The permuted variable importance is comparable with the measure defined in the classical random forest algorithm. It is computed using the out of bag (oob) sample for the tree $k\;\;(B^{(k)})$ for each $X_j$ predictor variable.  Then the
% permuted importance of the variable $X_j$ in the tree $k$ can be defined as:
% 
% \[
% IMP^{(k)}(X_j) = \frac{\sum_{i \in B^{(k)} } I(y_i=\hat y_i^{(k)})-I(y_i=\hat y_{i,P_j}^{(k)})}{|B^{(k)}|}
% \]
% 
%  where $\hat y_i^{(k)}$
% is the predicted class for the observation $i$ in the tree $k$ and $y_{i,P_j}^{(k)}$ is the predicted class for the observation $i$ in the tree $k$ after permuting the values for variable $X_j$. The global permuted importance measure is the average importance over all the trees in the forest.
% This measure is based on comparing the accuracy of classifying out-of-bag observations, using the true class with permuted (nonsense) class.
% To compute this measure you should use permute\_importance function.
% 
% \begin{figure}
% <<fig.show='hold', fig.width = 5, fig.height = 5, echo=FALSE>>=
% impo1 <- permute_importance(pprf.crab) 
% impo1 
% 
% ggplot(impo1, aes(x = imp, y = nm)) + geom_point() 
% 
%   @
%   \end{figure}
%  This function returns a data frame with permuted importance measures, imp is the permuted importance measure defined in Brieman paper, imp2 is the permuted importance measure defined in randomForest package, the standard deviation (sd.im and sd.imp2) for each measure is computed and the also the standardized measure. 
% 
% For the second importance measure, the coefficients of each projection are examined. The magnitude of these values indicates importance, if the variables have been standardized. The variable importance for a single tree is computed by a weighted sum of the absolute values of the coefficients across nodes. The weights takes the number of classes in each node into account~\citep{lee2013pptree}. 
% Then the importance of the variable $X_j$ in the PPtree $k$ can be defined as: 
%  \[ 
%  IMP_{pptree}^{(k)}(X_j)=\sum_{nd = 1}^{nn}\frac{|\alpha_{nd}^{(k)}|}{cl_{nd} } 
%  \] 
%  Where $\alpha_{nd}^{(k)}$ is the projected coefficient for node $ns$ and variable $k$ and $nn$ the total number of node partitions in the tree $k$.
% 
% The global variable importance in a PPforest then can be defined in different ways. The most intuitive is the average variable importance from each PPtree across all the trees in the forest.
%  \[
%  IMP_{ppforest1}(X_j)=\frac{\sum_{k=1}^K IMP_{pptree}^{(k)}(X_j)}{K} 
% \] 
%  Alternatively we have defined a global importance measure for the forest as a weighted mean of the absolute value of the projection coefficients across all nodes in every tree. The weights are based on the projection pursuit indexes in each node ($Ix_{nd}$), and 1-(OOB-error of each tree)($acc_k$).
% 
% \[IMP_{ppforest2}(X_j)=\frac{\sum_{k=1}^K acc_k \sum_{nd = 1}^{nn}\frac{Ix_{nd}|\alpha_{nd}^{(k)}|}{nn }}{K}
% \]
% 
% <<>>=
% impo2 <-  ppf_avg_pptree_imp(pprf.crab, "Type")
% impo2 
% ggplot(impo2, aes(x = mean, y = variable) ) +geom_point() 
% @
%  Finally you can get the last importance measure we have proposed for the PPforest using ppf\_global\_imp function.
% \begin{figure}
% <<fig.show='hold', fig.width = 5, fig.height = 5, echo=FALSE>>=
% impo3 <- ppf_global_imp(data = crab, class = "Type", pprf.crab)  
% impo3
% ggplot(impo3, aes(x = mean, y = variable) ) + geom_point() 
% @
% \end{figure}
% If we compare the results with the `randomForest` function for this data set the results are the following: 
% 
% <<>>=
% rf.crab <- randomForest::randomForest(Type~., data = crab, proximity = TRUE, ntree = 100) 
% rf.crab
% @
% We can see that for this data set the `PPforest` performance is much better than using \pkg{randomForest}. \pkg{PPforest} works well since the classes can be separated by linear combinations of variables. 
% This is a clear case where oblique hyperplanes are more adequate in this case than hyperplanes horizontal to the axis. 
% 
%  Using the information available in the PPforest object, some visualization can be done. I will include some useful examples to visualize the data and some of the most important diagnostics in a forest structure. 
%  
% To describe the data structure a parallel plot can be done, the data were stundarized and the color represents the class variable.
% \begin{figure}
% <<fig.show='hold',fig.width = 5 ,fig.height = 4, warning=FALSE, echo=FALSE>>=
%  
% parallel <- function(ppf){ 
% myscale <- function(x) (x - mean(x)) / sd(x) 
% 
% scale.dat <- ppf$train %>% dplyr::mutate_each(dplyr::funs(myscale),-dplyr::matches(ppf$class.var)) 
% scale.dat.melt <- scale.dat %>%  dplyr::mutate(ids = 1:nrow(ppf$train)) %>% tidyr::gather(var,Value,-Type,-ids) 
%  scale.dat.melt$Variables <- as.numeric(as.factor(scale.dat.melt$var)) 
%  colnames(scale.dat.melt)[1] <- "Class" 
%  ggplot2::ggplot(scale.dat.melt, ggplot2::aes(x = Variables, y = Value, 
%                           group = ids, key = ids, colour = Class, var = var)) +
%  ggplot2::geom_line(alpha = 0.3) + ggplot2::scale_x_discrete(limits = levels(as.factor(scale.dat.melt$var)), expand = c(0.01,0.01)) + 
%   ggplot2::ggtitle("Data parallel plot ") + ggplot2::theme(legend.position = "none", axis.text.x  = element_text(angle = 90, vjust = 0.5)) + 
%   ggplot2::scale_colour_brewer(type = "qual", palette = "Dark2") 
% }
% parallel(pprf.crab)
% @
% \end{figure}
% Some  auxiliary functions are available in `PPforest` to get the data structure needed to do some visualization.  Because the PPforest is composed of many tree fits on subsets of the data, a lot of statistics can be calculated to analyze as a separate data set, and better understand how the model is working. -->
% Some of the diagnostics of interest are: variable importance, OOB error rate, vote matrix and proximity matrix. Also will be possible to explore the individual model level. 
% 
% With a decision tree we can compute for every pair of observations the proximity matrix. This is a $nxn$ matrix where if two cases $k_i$ and $k_j$ are in the same terminal node increase their proximity by one, at the end normalize the proximities by dividing by the number of trees
% To visualize the proximity matrix we use a scatter plot with information from multidimensional scaling method. In this plot color indicates the true species and sex. For this data two dimensions are enough to see the four groups separated quite well. Some crabs are clearly more similar to a different group, though, especially in examining the sex differences. 
%  
%  \begin{figure}
% <<fig.show='hold',fig.width = 5 ,fig.height = 4, warning=FALSE, echo=FALSE>>=
% mdspl2d <- function(ppf, lege = "bottom", siz = 3, k = 2) { 
%  d <- diag(nrow(ppf$train))
%  d <- as.dist(d + 1 - ppf$proximity) 
%  rf.mds <- stats::cmdscale(d, eig = TRUE,  k = k) 
%  colnames(rf.mds$points) <- paste("MDS", 1:k, sep = "")
%  df <- data.frame(Class = ppf$train[, 1], rf.mds$points) 
%   mds <- ggplot2::ggplot(data = df) + 
%   ggplot2::geom_point(ggplot2::aes(x = MDS1, y = MDS2, color = Class), 
%             size = I(siz), alpha = .5) + 
%  ggplot2::scale_colour_brewer(type = "qual", palette = "Dark2", name = "Class") + 
%  ggplot2::theme(legend.position = lege, aspect.ratio = 1) 
%   mds 
% } 
% 
%  mdspl2d(pprf.crab) 
%  @
%  \end{figure}
%  The vote matrix ($n \times p$) contains the proportion of times each observation was classified to each class, whole oob. Two possible approaches to visualize the vote matrix information are shown, with a side-by-side jittered dot plot or with ternary plots. 
%  A side-by-side jittered dotplot is used for the display, where class is displayed on one axis and proportion is displayed on the other. For each dotplot, the ideal arrangement is that points of observations in that class have values bigger than 0.5, and all other observations have less. This data is close to the ideal but not perfect, e.g. there are a few blue male crabs (orange) that are frequently predicted to be blue females (green), and a few blue female crabs predicted to be another class. 
% \begin{figure}
%  <<fig.show='hold',fig.width = 5 ,fig.height = 5, warning = FALSE, echo=FALSE>>=
%  side <-  function(ppf, ang = 0, lege = "bottom", siz = 3,
%                    ttl = "Side by side dotplot") { 
%  voteinf <- data.frame(ids = 1:length(ppf$train[, 1]), Type = ppf$train[, 1],
%                       ppf$votes, pred = ppf$prediction.oob ) %>% 
%   tidyr::gather(Class, Probability, -pred, -ids, -Type) 
%    ggplot2::ggplot(data = voteinf, ggplot2::aes(Class, Probability, color = Type)) + 
%  ggplot2::geom_jitter(height = 0, size = I(siz), alpha = .5) +
%     ggtitle(ttl) + 
%     ylab("Proportion") + 
%     ggplot2::scale_colour_brewer(type = "qual", palette = "Dark2") + 
%    ggplot2::theme(legend.position = lege, legend.text = ggplot2::element_text(angle = ang)) + 
%    ggplot2::labs(colour = "Class")
%  }
%  
%  side(pprf.crab) 
% @
% \end{figure}
%  A ternary plot is a triangular diagram that shows the proportion of three variables that sum to a constant and is done using barycentric coordinates. Compositional data lies in a $(p-1)$-D simplex in $p$-space. 
%  One advantage of ternary plot is that are good to visualize compositional data and the proportion of three variables in a two dimensional space can be shown. 
%  When we have tree classes a ternary plot are well defined. With more than tree classes the ternary plot idea need to be generalized. \cite{sutherland2000orca} suggest the best approach to visualize compositional data will be to project the data into the $(p-1)-$D space (ternary diagram in $2-D$)  This will be the approach used to visualize the vote matrix information.  
%  A ternary plot is a triangular diagram used to display compositional data with three components. More generally, compositional data can have any number of components, say $p$, and hence is contrained to a $(p-1)$-D simplex in $p$-space. The vote matrix is an example of compositional data, with $G$ components. 
% 
% \begin{figure}
% <<fig.show='hold',fig.width = 7 ,fig.height = 4, warning=FALSE, echo=FALSE>>=
%  pl_ter <- function(dat, dx, dy ){ 
%  p1  <- dat[[1]] %>% dplyr::filter(pair %in% paste(dx, dy, sep = "-") ) %>% 
%   dplyr::select(Class, x, y) %>% 
%  ggplot2::ggplot(aes(x, y, color = Class)) + 
%  ggplot2::geom_segment(data = dat[[2]], aes(x = x1, xend = x2, 
%                                 y = y1, yend = y2), color = "black" ) + 
%   ggplot2::geom_point(size = I(3), alpha = .5) + 
%    ggplot2::labs(y = " ",  x = " ") + 
%    ggplot2::theme(legend.position = "none", aspect.ratio = 1) + 
%  ggplot2::scale_colour_brewer(type = "qual", palette = "Dark2") + 
%    ggplot2::labs(x = paste0("T", dx, ""), y = paste0("T", dy, " ")) + 
%   ggplot2::theme(aspect.ratio = 1) 
%  p1 
%  }
%   p1 <-  pl_ter(ternary_str(pprf.crab, id = c(1, 2, 3), sp = 3, dx = 1, dy = 2), 1, 2 ) 
%  p2 <-  pl_ter(ternary_str(pprf.crab, id = c(1, 2, 3), sp = 3, dx = 1, dy = 3), 1, 3) 
%  p3 <-  pl_ter(ternary_str(pprf.crab, id = c(1, 2, 3), sp = 3, dx = 2, dy = 3), 2, 3) 
%  
%  gridExtra::grid.arrange(p1, p2, p3, ncol = 3)
% @
% \end{figure}
% 
%  To see a complete description about how to visualize a PPforest object read Interactive Graphics for Visually Diagnosing Forest Classifiers in R (Include reference). 
% \begin{figure}
%  << fig.show = 'hold',fig.width = 5 ,fig.height = 4, warning = FALSE>>=
%  impotree <- function(data,ppf, nodes = c(1, 2, 3), tree ){ 
%  bnf <- function(x) { 
%    bn <- abs(x$projbest.node) 
%   bn[bn == 0] <- NA 
%    data.frame(node = 1:dim(x$projbest.node)[1], bn)
%  } 
% 
%  bestnode <- ppf[["output.trees"]] %>%  lapply(bnf) %>% dplyr::bind_rows() 
%  colnames(bestnode)[-1] <- colnames(data[, -1]) 
%  bestnode$node <- as.factor(bestnode$node)
%  aux <- bestnode %>% dplyr::filter(node %in% nodes) %>% dplyr::mutate(ids = rep(1:ppf$n.tree, each = length(nodes))) %>% tidyr::gather(var, value, -ids, -node)
% aux$Variables <- as.numeric(as.factor(aux$var) ) 
%  aux$Abs.importance <- round( aux$value, 2) 
%  p <- ggplot2::ggplot(dplyr::filter(aux, !ids %in% tree), 
%      ggplot2::aes(x = Variables , y = Abs.importance , group = ids)) +    ggplot2::geom_jitter(height = 0, size = I(3), alpha = 0.3) + 
%   ggplot2::facet_grid(node ~ .) + ggplot2::scale_x_discrete(limits = levels(as.factor(aux$var)) ) + ggplot2::theme(legend.position = "none") 
%     p + ggplot2::geom_jitter(
%    data = dplyr::filter(aux, ids %in% tree), 
%    ggplot2::aes( 
%     x = Variables , 
%     y = Abs.importance ,
%     group = ids, 
%     colour = "red" 
%   ),
%    height = 0, 
%    size = I(3) 
%  ) + ggplot2::facet_grid(node ~ .)  + 
%   ggplot2::theme(legend.position = "none") + ggplot2::labs(y = "Absolute coefficient") 
%  } 
%  impotree(crab, pprf.crab, tree =1 ) 
% @
% \end{figure}
% \begin{figure}
% << fig.show = 'hold', fig.width = 5 ,fig.height = 4, warning = FALSE>>=
%  dat_pl <- PPforest::node_data(ppf = pprf.crab, tr = 1) 
%  nodes <- c(1,2,3) 
%   myColors <- RColorBrewer::brewer.pal( dim( unique(pprf.crab$train[pprf.crab$class.var]))[1], "Dark2") 
%  names(myColors) <- levels(pprf.crab$train[pprf.crab$class.var][, 1]) 
%  dat_pl$Class <- as.factor(dat_pl$Class) 
%    levels(dat_pl$Class) <-  levels(pprf.crab$train[pprf.crab$class.var][, 1]) 
%   dat_pl %>% 
%           dplyr::filter(node.id %in% unique(node.id)[nodes]) %>% 
%         ggplot2::ggplot(aes(  x = proj.data, group = Class, fill = Class )) + 
%          ggplot2::geom_density(alpha = .5) + 
%           ggplot2::facet_grid(~ node.id, scales = 'free') + 
%          ggplot2::scale_fill_manual("", values = myColors) + 
%          ggplot2::geom_vline(ggplot2::aes(xintercept = cut), 
%                  linetype = "dashed", 
%                  color = 2) + ggplot2::xlab("") + ggplot2::theme(legend.position = "bottom")
% @
% \end{figure}
% \begin{figure}
% <<fig.show = 'hold', fig.width = 5 ,fig.height = 4, warning = FALSE>>=
%   dat_mosaic <-
%  data.frame(with(dat_pl, table(Class, Dir, node.id))) 
%     p1 <- dat_mosaic %>% dplyr::filter(node.id %in% unique(node.id)[nodes]) %>% 
%      ggplot2::ggplot() + ggmosaic::geom_mosaic(ggplot2::aes( 
%     weight = Freq, 
%         x = ggmosaic::product(Class, Dir), 
%       fill = Class 
%       )) + ggplot2::facet_grid( ~ node.id) + 
%       ggplot2::scale_fill_manual("", values = myColors) + 
%       xlab("Class") + ggplot2::theme( 
%          legend.position = "bottom", 
%          axis.text.x  = element_text(angle = 90, vjust = 0.5),aspect.ratio = 1 
%       ) 
%     p1 
% @
% \end{figure}
%  From PPforest object we can plot a heat map of the proximity matrix using the function pproxy\_plot. 
% This function has tree arguments, ppf is a PPforest object, type an argument that specify if the plot is a heatmap or a MDS plot.
% If the plot is a MDS plot the argument k defines the number of MDS layouts. 
%  This function return an interactive plot based on plotly` package if `interactive = TRUE`. -->
% 
% 
% <!-- ```{r,fig.show='hold',fig.width = 5 ,fig.height = 4, warning=FALSE} -->
% 
% <!-- PPforest::pproxy_plot(pprf.crab, type = "heat", interactive = FALSE) -->
% 
% <!-- ``` -->
% 
% <!-- In this plot we can see a heat map for the proximity matrix, we can observe that strong red color indicates that the observations are more similar. -->
% <!-- The data are ordered by class (BlueFemale, BlueMale, OrangeFemale and OrangeMale), in the heat map we can observe a colored block diagonal structure, this means that the observations from the same class are similar here the same class were classified most of the time in the correct class, but also we can observe that observations from BlueMale and BlueFemale are similar too then some data were classified in the incorrect class. -->
% 
% 
% <!-- Additionally `pproxy_plot` can be used to plot the MDS using proximity matrix information. -->
% 
% <!-- If we select k =2 the output plot is as follows: -->
% 
% <!-- ```{r,fig.show='hold',fig.width = 6 ,fig.height = 4, warning = FALSE} -->
% <!-- PPforest::pproxy_plot(pprf.crab, type="MDS", k =2, interactive = FALSE ) -->
% <!-- ``` -->
% 
% 
% <!-- We can observe a spatial separation between classes. Orange (male and female) are more separated than Blue (male and female). -->
% 
% <!-- If we select k>2,  we can observe that using two dimensions is enough to see the spatial separation. -->
% 
% 
% <!-- ```{r,fig.show='hold',fig.width = 6 ,fig.height = 6, warning=FALSE} -->
% <!-- PPforest::pproxy_plot(pprf.crab, type="MDS",k = 3, interactive = TRUE) -->
% <!-- ``` -->
% 
% <!-- Another possible visualization in `PPforest` package is for the importance measure. -->
% 
% <!-- The variable importance for the group separation can be measured by the projection coefficients in each individual tree. Based on these coefficients we can examine how the classes are separated and which variables are more relevant for the separation. -->
% 
% <!--  In `PPtree` the projection coefficient of each node represent the importance of variables to class separation in each node. Since in `PPforest` we have `m` trees we can define a global importance measure.  For this global importance measure we need to take into account the importance in each node and combine the results for all the trees in the forest. The importance measure of `PPforest` is a  weighted mean of the absolute value of the projection coefficients across all nodes in every tree. The weights are  the projection pursuit index in each node, and 1-the out of bag error of each tree. -->
% 
% <!-- `ppf_importance` has four arguments, data (data frame with the complete data set), class (a character with the name of the class variable), global ( logical that indicate is the importance measure is global or not) and weight (logical argument that indicates if the importance measure is weighted or not). -->
% 
% 
% <!-- Using the `ppf_importance` function we can plot the weighted global importance measure in `PPforest`. -->
% 
% <!-- ```{r, fig.show='hold',fig.width = 5 ,fig.height = 4, warning=FALSE} -->
% <!-- PPforest::ppf_importance(data = crab, class = "Type", pprf.crab, global = TRUE, weight = TRUE, interactive = FALSE) -->
% <!-- ``` -->
% 
% <!-- We can see that the most important variable in this example is RW while the less important is BD. -->
% <!-- An importance measure for each node also is available. -->
% 
% 
% <!-- ```{r, fig.show='hold',fig.width = 5 ,fig.height = 4, warning=FALSE} -->
% <!-- PPforest::ppf_importance(data = crab, class = "Type", pprf.crab, global = FALSE, weight = TRUE, interactive = FALSE) -->
% <!-- ``` -->
% 
% <!-- The importance variable order is the same for node 1 and node 2 but different in node 3. -->
% 
% <!-- Finally a cumulative out of error plot can be done using `ppf_oob_error` -->
% <!-- This function has three arguments, ppf (`PPforest` object), nsplit1 (number, increment of the sequence where cumulative oob error rate is computed in the  1/3 trees) and interactive if we want a interactive plot. -->
% 
% 
% <!-- ```{r, fig.show='hold',fig.width = 6 ,fig.height = 4, warning=FALSE} -->
% <!--  PPforest::ppf_oob_error(pprf.crab, nsplit1 = 15, interactive = FALSE) -->
% <!-- ``` -->
% 
% <!-- The oob-error rate decrease when we increase the numbers of trees but gets constant with less than 70 trees. -->
% 
% 
% 
% <!--        a <- ggplot2::ggplot(import.vi.wg, ggplot2::aes(x = mean, y = variable)) + ggplot2::geom_point() + ggplot2::theme(aspect.ratio=1) -->
% <!--        print(import.vi.wg) -->
% 
% <!--      plotly::ggplotly(a) -->
