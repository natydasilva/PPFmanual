\section*{Abstract}
The \pkg{PPforest} package (short for Projection pursuit classification random forest) implements this new ensemble learning method introduced in \cite{dasilvappforest}. 
In the PPforest, each split is based on a linear combination of randomly chosen variables. The linear combination is computed by optimizing a projection pursuit index, to get a projection of the variables that best separates the classes. The PPforest uses the \pkg{PPtree} algorithm, which fits a single tree to the data. Utilizing linear combinations of variables to separate classes takes the correlation between variables into account, and can outperform the basic forest when separations between groups occur on combinations of variables. Two projection pursuit indexes, LDA and PDA, are used for PPforest.
To improve the speed performance of PPforest package, the main functions were implemented in Rcpp, that implies the original PPtree algorithm was translated and other function which required to be fast.
PPforest package utilizes a number of R packages some of them included in ``suggests'' not to load them all at package start-up.
In this user manual, the key functions included in the package are presented and some visualizations using PPforest diagnostics.

\section{Package description and illustrative examples}
In this section, a detailed description of \pkg{PPforest} package along with examples is presented. To get detailed information about PPforest method check \cite{dasilvappforest}
\pkg{PPforest} package implements a classification random forest using projection pursuit classification trees.

\subsection{Functions and data}
Table \ref{tabfun} shows the complete list of functions included in the package along with a brief description. 

 \begin{center}
 \begin{table}[!hbpt]
  \caption{Summary of functions implemented in \pkg{PPforest}.\label{tabfun}}
  \begin{tabular}{ l | p{12cm} } \hline
\textbf{Function} &\textbf{Description} \\ \hline
 {\tt baggtree}&For each bootstrap sample grow a projection pursuit tree (PPtree object).\\ \\ \hline
 {\tt node\_data} &Data structure with the  projected and boundary by node and class.\\ \\ \hline
 {\tt PPclassify2}&Predict class for a test dataset and calculate prediction error.\\ \\ \hline
 {\tt PPforest} &Runs a Projection pursuit random forest.\\ \\ \hline
 {\tt permute\_importance}&Obtain the permuted importance variable measure.\\ \\ \hline
 {\tt PPtree\_split}&Projection pursuit classification tree with random variable selection in each split.\\ \\ \hline
 {\tt ppf\_avg\_pptree\_imp}& Computes a global importance measure for a PPforest object, average.\\ \\ \hline
 {\tt ppf\_global\_imp}& Computes a global importance measure for a PPforest object.\\ \\ \hline
 {\tt print.PPforest}& Print PPforest object\\ \\ \hline
 {\tt predict.PPforest}&Predict class for the test set and calculate prediction error\\ \\ \hline
 {\tt ternary\_str}&Data structure with the  projected and boundary by node and class\\ \\ \hline
 {\tt tree\_pred}&Obtain predicted class for new data using PPforest.\\ \\ 
   \hline
  \end{tabular}
  \end{table}
\end{center}


The developed version for the package is available on GitHub and will be soon on CRAN. To install \pkg{PPforest} you should run the following code:
<<echo = TRUE, message=FALSE, warning=FALSE,eval=FALSE>>=
library(devtools)
install_github("natydasilva/PPforest")
library(PPforest)
@
Australian crab data set will be used as an example in this manual. This data contains measurements on rock crabs of the genus Leptograpsus. There are 200 observations from two species (blue and orange) and for each species (50 in each one) there are 50 males and 50 females. The class variable has 4 classes with the combinations of species and sex (BlueMale, BlueFemale, OrangeMale, and OrangeFemale). The data were collected on site at Fremantle, Western Australia. For each specimen, five measurements were made, using vernier calipers.

\begin{itemize}
\item FL the size of the frontal lobe length, in mm
\item RW rear width, in mm
\item CL length of mid line of the carapace, in mm
\item CW maximum width of carapace, in mm
\item BD depth of the body; for females, measured after displacement of the abdomen, in mm
\end{itemize}

 A scatterplot matrix using \pkg{GGally} \citep{ggally} package is used to visualize crab data set presented in  Figure \ref{scatercrab}.  This figure shows a strong, positive and linear association between the different variables. Also look like the classes can be separated by linear combinations of variables. This example is a case where PPforest outperforms the classic Random Forest because of the linear projections in each node partitions takes into account the correlation between variables.

<< echo=FALSE, message=FALSE, warning=FALSE>>=
require(PPforest)
require(RColorBrewer)
require(GGally)
require(gridExtra)
require(PPtreeViz)
require(plotly)
require(dplyr)
library(ggplot2)
library(ggmosaic)
@
\begin{figure}[!hbpt]
<< echo=FALSE, fig.height=8, fig.width=8, message=FALSE>>=

a <- ggpairs(PPforest::crab,
    columns = 2:6,
    ggplot2::aes(colour = Type, alpha = .1),
    lower = list(continuous = 'points'),
    axisLabels = 'none',
    upper = list(continuous='blank')
     , legend = NULL)

a
@
\caption{Scatter plot matrix for crab data \label{scatercrab}}
\end{figure}
\pkg{PPforest} includes some data set that was used to test its predictive performance. The data sets included are summarized in Table. \ref{datappf}.

\begin{center}
\begin{table}[!hbpt]
 \caption{Summary of data included in \pkg{PPforest} package\label{datappf}.}
\begin{tabular}{ l | p{12cm} } \hline
\textbf{Data} & \textbf{Description}\\ \hline
{\tt crab} & Measurements on rock crabs of the genus Leptograpsus. \\ \\ \hline
{\tt fishcatch} & Measurements on fishes caught form Finland.\\ \\ \hline
{\tt glass} & Measurements on 6 different types of class.\\ \\ \hline
{\tt image} & Instances from 7 outoor images.\\ \\ \hline
{\tt leukemia} & Gene expression data set in two types of acute leukemias.\\ \\ \hline
{\tt lymphoma} & Gene expression in the three most prevalent adult lymphoid malignancies.\\ \\ \hline
{\tt NCI60} & Gene expression data among the 60 cell lines.\\ \\ \hline
{\tt parkinson} & Data set containing 195 observations from 2 parkinson types. \\ \\ \hline
{\tt wine} & Data set containing observations from 3 wine grown cultivares in Italy.\\  \\ \hline
  \end{tabular}
  \end{table}
\end{center}



\subsection{{\tt PPforest}}

The main function of the package is called {\tt PPforest} which implements a projection pursuit random forest. Table \ref{ppforestfun} presents the {\tt PPforest} function arguments and their respective description.
  \begin{center}
 \begin{table}[!hbpt]
 \caption{Summary of available arguments of function {\tt PPforest} \label{ppforestfun}.}
  \begin{tabular}{ l | p{12cm} } \hline
  \textbf{Arguments} & \textbf{Description}\\ \hline
{\tt data}	& Data frame with the complete data set\\ \hline
{\tt class}	& A character with the name of the class variable.\\ \hline
{\tt std} & If TRUE standardize the data set, needed to compute global importance measure\\ \hline
{\tt size.tr} &Size proportion of the training if we want to split the data in\\ &training and test\\ \hline
 {\tt m}&Number of bootstrap replicates, this corresponds with the number of trees to\\ 
 &grow. To ensure that each observation is predicted a few times we have to select\\ 
 &this number no too small. m = 500 is by default\\ \hline
{\tt PPmethod} & Projection pursuit index to optimize in each classification tree. The options are\\
&LDA and PDA, linear discriminant and penalized linear discriminant. By default it is LDA\\ \hline
{\tt size.p} & Proportion of variables randomly sampled in each split\\ \hline
 {\tt lambda}& Penalty parameter in PDA index and is between 0 to 1. If lambda = 0, no penalty\\
 &parameter is added and the PDA index is the same as LDA index. If lambda = 1\\
 &all variables are treated as uncorrelated. The default value is lambda = 0.1\\ \hline
 {\tt parallel} &
Logical condition, if it is TRUE then parallelize the function\\ \hline
 {\tt cores} &Number of cores used in the parallelization\\ \hline
  \end{tabular}
  \end{table}
  \end{center}
  
{\tt PPforest} function runs a projection pursuit random forest. The arguments are a data frame with the data information, class with the name of the class variable argument.  {\tt size.tr} to specify the proportion of observations using in the training. Using this function we have the option to split the data in training and test using {\tt size.tr} directly. {\tt size.tr} is the proportion of data used in the training and the test proportion will be 1- {\tt size.tr}.
The number of trees in the forest is specified using the argument {\tt m}. The argument {\tt size.p} is the sample proportion of the variables used in each node split, {\tt PPmethod} is the projection pursuit index to be optimized,  two options LDA and PDA are available.
The algorithm can be parallelized specifying {\tt parallel =TRUE} and the number of used cores can be included in {\tt core} core argument.

The following code is to run {\tt PPforest} using crab data, in this case all the observations available are used to run the forest ({\tt size.tr = 1}), the number of trees is 200 ( {\tt m= 200})  and the proportion of variables used in each node partitions is .5 ({\tt size.p = .5}) in this case 2 variables). The selected projection index is `LDA' ({\tt PPmethod = 'LDA'}) and the forest is parallelized ({\tt parallel = TRUE}) specifieng two cores ({\tt cores = 2}).

<<echo=TRUE,>>=
set.seed(146)
pprf.crab <- PPforest::PPforest(data = crab, class = "Type", size.tr = 1, 
                                m = 200, size.p =  .5,  PPmethod = 'LDA',  
                                parallel =TRUE, cores = 2)
@

{\tt PPforest} print a summary result from the model with the confusion matrix information and the oob-error rate in a similar way \pkg{randomForest} packages does, these makes them comparable. Based on confusion matrix, we can observe that the biggest error is for BlueMale class. Most of the wrong classified values are between BlueFemale and BlueMale.

<<echo=TRUE>>=
pprf.crab
@

This function returns the predicted values of the training data, training error, test error and predicted test values. Also there is the information about out of bag error for the forest and also for each tree in the forest. Bootstrap samples, output of all the trees in the forest from , proximity matrix and vote matrix, number of trees grown in the forest, number of predictor variables selected to use for splitting at each node. Confusion matrix of the prediction (based on OOb data), the training data and test data and vote matrix are also returned.

<<echo=TRUE>>=
 str(pprf.crab, max.level=1, list.len=5)
@
\begin{center}
 \begin{table}[!hbpt]
\caption{Summary of available values of function {\tt PPforest} \label{ppforestout}.}
  \begin{tabular}{ l | p{12cm} } \hline
  \textbf{Value} & \textbf{Description}\\ \hline
{\tt prediction.training}	& Predicted values for training data se\\  \hline
{\tt training.error}	&Error of the training data set.\\  \hline
{\tt prediction.test} & Predicted values for the test data set if testap = TRUE(default\\ \hline
{\tt oob.error.forest} & Out of bag error in the forest\\  \hline
 {\tt oob.error.tree}&Out of bag error for each tree in the forest\\ \hline
{\tt boot.samp} & Information of bootrap samples.\\ \hline
{\tt output.trees} & Output from a {\tt trees\_pp} for each bootrap sample\\  \hline
 {\tt proximity}& Proximity matrix, if two cases are classified in the same terminal node then the proximity matrix is increased by one in PPforest there are one terminal node per class\\  \hline
 {\tt votes} & A matrix with one row for each input data point and one column for each class, giving the fraction of (OOB) votes from the PPforest\\  \hline
 {\tt n.tree} &Number of trees grown in PPforest\\  \hline
 {\tt n.var} &Number of predictor variables selected to use for spliting at each node\\  \hline
 {\tt type} &Classification\\  \hline
 {\tt confusion} &Confusion matrix of the prediction (based on OOb data)\\  \hline
 {\tt call} & The original call to PPforest. \\  \hline
 {\tt train}& Is the training data based on size.tr sample proportion\\  \hline
 {\tt test}& Is the test data based on $1-${\tt size.tr} sample proportion\\  \hline
  \end{tabular}
  \end{table}
  \end{center}

\subsection{{\tt baggtree}}
This function grows a projection pursuit tree for each bootstrap sample and random sample selection in each node partition.
A summary of the arguments and a description for this function is presented in Table \ref{baggtreefun}.
 \begin{center}
 \begin{table}[!h]
 \caption{Summary of available arguments of function {\tt baggtree} \label{baggtreefun}.}
  \begin{tabular}{ l | p{12cm} } \hline
  \textbf{Arguments} & \textbf{Description}\\ \hline
{\tt data} &	Data frame with the complete data set\\ \hline
{\tt class} &	A character with the name of the class variable \\ \hline
{\tt m} &	Number of bootstrap replicates, this corresponds with the number of trees to grow. To ensure that each observation is predicted a few times we have to select this number no too small. m = 500 is by default\\ \hline
{\tt PPmethod} &	Projection pursuit index to be optimized, options LDA or PDA, by default it is LDA\\ \hline
{\tt lambda} &	A parameter for PDA index \\ \hline
{\tt size.p} & Proportion of random sample variables in each split \\ \hline
{\tt parallel} &	Logical condition, if it is TRUE then parallelize the function \\ \hline
{\tt cores} & 	Number of cores used in the parallelization \\ \hline
  \end{tabular}
  \end{table}
  \end{center}

This function can be used directly but it is one of the main functions needed to run the forest. 
<<echo=TRUE>>=
crab.trees <- baggtree(data = crab, class = "Type",
m =  200, PPmethod = 'LDA', lambda = .1, size.p = 0.5 , parallel = TRUE,
cores = 2)
str(crab.trees[[1]], max.level = 1)

@


\subsection{{\tt node\_data}}
This functions stucture data and contain useful information to visualize {\tt PPforest} results. The information contained is projected data and boudary bu node and class.
Table \ref{nodedatfun} contains a summary whit the main argument described for {\tt node\_data}.
 \begin{center}
 \begin{table}[!hbpt]
 \caption{Summary of available arguments of function {\tt node\_data} \label{nodedatfun}.}
  \begin{tabular}{ l | p{12cm} } \hline
  \textbf{Arguments} & \textbf{Description}\\ \hline
{\tt ppf} &	Is a PPforest object\\ \hline
{\tt tr} &	Numerical value to identify a tree\\ \hline
{\tt Rule} & 	Split rule 1:mean of two group means, 2:weighted mean, 3: mean of max(left group) and min(right group), 4: weighted mean of max(left group) and min(right group) \\ \hline
  \end{tabular}
  \end{table}
  \end{center}
  
This function can be implenented as follows, in this case the PPforest object ({\tt ppf}) is ppf.crab, the selected tree is 1 ({\tt tr =1}) and the rule by default is 1 ({\tt Rule = 1}). 
<<echo=TRUE>>=
node_data(ppf = pprf.crab, tr = 1)  %>% head()
@

\begin{center}
 \begin{table}[!h]
 \caption{Summary of available arguments of function {\tt node\_data} \label{nodedatout}.}
  \begin{tabular}{ l | p{12cm} } \hline
  \textbf{Value} & \textbf{Description}\\ \hline
{\tt proj.data}	&Projected data for each node and class\\ \hline
{\tt Class} & 	Original class for each node. \\ \hline
{\tt cut} &	Split value for each node partition.\\ \hline
{\tt node.id} & 	Node identifier\\ \hline
{\tt LR.class } & Node side information, L indicates left node and R indicates right node. \\ \hline
{\tt Dir} & Direction.\\ \hline
 \end{tabular}
  \end{table}
  \end{center}

This function is useful to visualize PPtree results because it makes available the projected information for each class and node. Figure \ref{densi} shows a density plot for tree 1 in the forest and for three nodes. For a further explanation about visualization for PPforest objects check \cite{da2017interactive}.
\begin{figure}[!hbpt]
<< echo=TRUE, fig.height=4, fig.width=8, message=FALSE>>=
dat_pl <- node_data(ppf = pprf.crab, tr = 1)
nodes <- c(1,2,3)
myColors <- brewer.pal( 
  dim( unique(pprf.crab$train[pprf.crab$class.var]))[1], "Dark2")
names(myColors) <- levels(pprf.crab$train[pprf.crab$class.var][, 1])
dat_pl$Class <- as.factor(dat_pl$Class)
levels(dat_pl$Class) <-  levels(pprf.crab$train[pprf.crab$class.var][, 1])
dat_pl %>%
  filter(node.id %in% unique(node.id)[nodes]) %>%
  ggplot(aes(  x = proj.data, group = Class, fill = Class )) +
  geom_density(alpha = .5) +
  facet_grid(~ node.id, scales = 'free') +
  scale_fill_manual("", values = myColors) +
  geom_vline(aes(xintercept = cut),
             linetype = "dashed",
             color = 2) + xlab("") + theme(legend.position = "bottom")
@
\caption{Visualizing the PPtree model of the crab data. The tree has three nodes (top). The density plots show the data projections at each node, colored by group (middle). The dashed vertical red line indicates the split value of each node. At node 1 the blue species is separated from orange species. Nodes 2 and 3 separate the sexes, which are more confused for the blue species.\label{densi}}
\end{figure}

Figure \ref{mos} shows an additional visualization using the information from {\tt dats\_node} function. This figure is a mosaic plot for the confusion table for each split.
\begin{figure}[!hbpt]
<<echo=TRUE, fig.height=4, fig.width=8, message=FALSE>>=
dat_mosaic <-
  data.frame(with(dat_pl, table(Class, Dir, node.id)))
p1 <- dat_mosaic %>% filter(node.id %in% unique(node.id)[nodes]) %>%
  ggplot() + geom_mosaic(aes(
    weight = Freq,
    x = product(Class, Dir),
    fill = Class
  )) + facet_grid( ~ node.id) +
  scale_fill_manual("", values = myColors) +
  xlab("Class") + theme(
    legend.position = "bottom",
    axis.text.x  = element_text(angle = 90, vjust = 0.5), aspect.ratio = 1
  )
p1
@
\caption{Mosaic plots of the confusion table for each split. Node 1 shows the clear split of the species, with a small number of misclassifications. Node 2 where orange females are separated from orange males indicates small number of misclassifications. Node 3 where blue females are separated from blue males, shows a larger misclassification than for orange specie.\label{mos}}
\end{figure}

\newpage

\subsection{{\tt PPtree\_split}}
{\tt PPtree\_split} function implements a projection pursuit classification tree with random variable selection in each split, based on the original PPtreeViz algorithm. This function returns a PPtreeclass object.
A summary of available arguments and its description is presented in Table \ref{ppsplitfun}
 \begin{center}
 \begin{table}[!h]
 \caption{Summary of available arguments of function {\tt PPtree\_split} \label{ppsplitfun}.}
  \begin{tabular}{ l | p{12cm} } \hline
  \textbf{Arguments} & \textbf{Description}\\ \hline
{\tt form} & 	A character with the name of the class variable\\ \hline
{ \tt data} &	Data frame with the complete data set \\ \hline
{ \tt PPmethod} &	Index to use for projection pursuit: `LDA', `PDA' \\ \hline
{\tt size.p} & 	Proportion of variables randomly sampled in each split, default is 1, returns a PPtree pbject\\ \hline
{\tt lambda} &	Penalty parameter in PDA index and is between 0 to 1 . If lambda = 0, no penalty parameter is added and the PDA index is the same as LDA index. If lambda = 1 all variables are treated as uncorrelated. The default value is lambda = 0.1\\ \hline
{\tt...}&	Arguments to be passed to methods\\ \hline
  \end{tabular}
  \end{table}
  \end{center}
To run one tree with random variables selection an example using crab data is presented bellow. In this case, LDA method is used and the number of variables used in each node partition is also 2 ({\tt size.p =0.5}).
<<echo=TRUE>>=
Tree.crab <-PPtree_split("Type~.", data = crab,
                                    PPmethod = "LDA", size.p = 0.5)
@

<<echo=TRUE>>=
str(Tree.crab, max.level = TRUE)
@

 \begin{center}
 \begin{table}[!h]
  \caption{Summary of output values of function {\tt PPtree\_split} \label{ppsplitfunout}.}
  \begin{tabular}{ l | p{12cm} } \hline
  \textbf{Values} & \textbf{Description}\\ \hline
{\tt Tree.Struct} & 	Tree structure of projection pursuit classification tree\\ \hline
{ \tt projbest.node} & 1-dim optimal projections of each split node \\ \hline
{ \tt splitCutoff.node} &	Cutoff values of each split node\\ \hline
{\tt origclass} & Original class\\ \hline
{\tt origdata} &	Original data\\ \hline
  \end{tabular}
   \end{table}
  \end{center}  
  
Since {\tt Tree.crab} is a PPtreeclass object all the visualization in \pkg{PPtreeViz} package can be implemented.
\begin{figure}[!hbpt]
<<echo=TRUE>>=
library(PPtreeViz)
plot(Tree.crab, font.size = 12, width.size = .8)
@
\caption{Tree structure plot using \pkg{PPtreeViz}}
\end{figure}

\newpage

\subsection{{\tt PPclassify2}}
This function predicts the class for the test set and compute the predicted error rate from a PPtree object.
Table \ref{ppclassfun} shows a summary with the arguments for {\tt PPclassify2} function.
 
  \begin{center}
 \begin{table}[!h]
  \caption{Summary of available arguments of function {\tt PPclassify2} \label{ppclassfun}.}
  \begin{tabular}{ l | p{12cm} } \hline
  \textbf{Argument} & \textbf{Description}\\ \hline
{\tt Tree.result} &	The result of PP.Tree.\\ \hline
{\tt test.data} &	The test dataset. \\ \hline
{\tt Rule} &	Split rule 1:mean of two group means, 2:weighted mean, 3: mean of max(left group) and min(right group), 4: weighted mean of max(left group) and min(right group)\\ \hline
{\tt true.class	} & True class of test dataset if available \\ \hline
  \end{tabular}
   \end{table}
  \end{center}  
This function returns the predicted error rate and the predicted values. 
<<echo=TRUE>>=
Tree.crab <- PPtree_split("Type~.", data = crab, PPmethod = "LDA", size.p = 0.5)
str(PPclassify2(Tree.crab))
@


\subsection{{\tt trees\_pred}}

 \begin{center}
 \begin{table}[!h]
 \caption{Summary of available arguments of function {\tt trees\_pred} \label{nodedatfun}.}
  \begin{tabular}{ l | p{12cm} } \hline
  \textbf{Arguments} & \textbf{Description}\\ \hline
{\tt object}	 &Trees classifiers from trees\_pp function or PPforest object\\ \hline
{\tt xnew}&	Data frame with explicative variables used to get new predicted values\\ \hline
{\tt parallel}	& Logical condition, if it is TRUE then parallelize the function\\ \hline
{\tt cores} &	Number of cores used in the parallelization\\ \hline
{\tt ...} &	Arguments to be passed to methods\\ \hline
 \end{tabular}
  \end{table}
  \end{center}
  
  This function returns a vector with the predicted values from {\tt baggtree} function or {\tt PPforest}.
  
<<echo = TRUE>>=
crab.trees <- baggtree(data = crab, class = "Type", 
m =  200, PPmethod = 'LDA', lambda = .1, size.p = 0.4 )
pr <- trees_pred(  crab.trees, xnew = crab[, -1], parallel = FALSE, 
                   cores = 2)

pprf.crab <- PPforest(data = crab, class = "Type",
 std = FALSE, size.tr = 2/3, m = 100, size.p = .4, PPmethod = 'LDA',
 parallel = TRUE )
 
pred <- trees_pred(pprf.crab, xnew = pprf.crab$test, paralle = TRUE)

@
\subsection{{\tt ternary\_str}}
This function returns a list with two elements need to plot a generalized ternary plot.
Table \ref{ternafun} presents a summary of the available arguments for this function.
 \begin{center}
 \begin{table}[!h]
 \caption{Summary of available arguments of function {\tt ternary\_str} \label{ternafun}.}
  \begin{tabular}{ l | p{12cm} } \hline
  \textbf{Arguments} & \textbf{Description}\\ \hline
{\tt ppf}	&Is a PPforest object\\ \hline
{\tt id} &	Is a vector with the selected projection directions\\ \hline
{\tt sp} &	Is the simplex dimensions, if k is the number of classes $sp = k - 1$\\ \hline
{\tt dx}&	First direction included in id\\ \hline
{\tt dy}	&Second direction included in id\\ \hline
 \end{tabular}
  \end{table}
  \end{center}
This example shows how to use the output from  {\tt ternary\_str} can be used to visualize a generalized ternary plot using vote matrix information.
This function returns a list with two elements need to plot a generalized ternary plot.
Table \ref{ternafun} presents a summary of the available arguments for this function.
\begin{figure}[!hbpt]
 <<echo=TRUE, fig.height=4, fig.width=8, message=FALSE>>=
pl_ter <- function(dat, dx, dy ){
  p1  <- dat[[1]] %>% dplyr::filter(pair %in% paste(dx, dy, sep = "-") ) %>%
    dplyr::select(Class, x, y) %>%
    ggplot2::ggplot(ggplot2::aes(x, y, color = Class)) +
    ggplot2::geom_segment(data = dat[[2]], ggplot2::aes(x = x1, xend = x2,
                                     y = y1, yend = y2), color = "black" ) +
    ggplot2::geom_point(size = I(3), alpha = .5) +
    ggplot2::labs(y = " ",  x = " ") +
    ggplot2::theme(legend.position = "none", aspect.ratio = 1) +
    ggplot2::scale_colour_brewer(type = "qual", palette = "Dark2") +
    ggplot2::labs(x = paste0("T", dx, " "), y = paste0("T", dy, " ")) +
    ggplot2::theme(aspect.ratio = 1)
  p1
}
#ternary plot in tree different selected dierections
p1<-pl_ter(ternary_str(pprf.crab, id = c(1,2,3), sp = 3, dx = 1, dy = 2), 1, 2)
p2<-pl_ter(ternary_str(pprf.crab, id = c(1,2,3), sp = 3, dx = 1, dy = 3), 1, 3)
p3<-pl_ter(ternary_str(pprf.crab, id = c(1,2,3), sp = 3, dx = 2, dy = 3), 2, 3)
gridExtra::grid.arrange(p1, p2, p3, ncol = 3)

@
\caption{Generalized ternary plot ((G-1)-D simplex, here it is a tetrahedron) representation of the vote matrix for four classes. The tetrahedron is shown pairwise. Each point corresponds to one observation and color is the true class. This is close but not a perfect classification, since the colors are concentrated in the corners and there are some mixed colors in each corner. }
\end{figure}
\newpage

\subsection{{\tt permute\_importance},  {\tt ppf\_avg\_pptree\_imp}  and {\tt ppf\_global\_imp}}
{\tt permute\_importance}, {\tt ppf\_avg\_pptree\_imp}  and {\tt ppf\_global\_imp} are three functions to compute importance measures for PPforest objects.

{\tt permute\_importance} this function returns a data frame with permuted importance measures, {\tt imp} is the permuted importance measure defined in \cite{breiman2001random}, {\tt imp2} is the prmuted importance measure defined in \pkg{randomForest} package, the standard deviation ({\tt sd.im} and {\tt sd.imp2}) for each measure is computed and also the stundardized mesure.

<<echo=TRUE>>=
permute_importance(ppf = pprf.crab) 
@

{\tt ppf\_avg\_pptree\_imp} this function computes the gobal importance measure for a PPforest object as the average IMP PPtree measure over all the trees in the forest.
Two arguments are needed; {\tt ppf} (is a PPforest object) and {\tt class} (a character with the nanme of the class variable).
This function returns a data frame with the average importance measuer for each class.

Finally  {\tt ppf\_global\_imp} this function returns a global importance measure for a PPforest pbject. The arguments for this functions are: {\tt data} (data frame with the complete data set), {\tt class} (a character with the name of the class variable), and {\tt ppf} (a PPforest object).

<<echo=TRUE>>=
ppf_global_imp(data = crab, class = "Type", pprf.crab) 

@
All these measures can be ploted for example:
\begin{figure}[!hbpt]
<<echo=TRUE, fig.height=4, fig.width=4, message=FALSE>>=
impo <- permute_importance(ppf = pprf.crab) 
ggplot(impo, aes(x = imp, y = nm)) + geom_point() 
@
\caption{Dot plot with the permuted importance measure for a PPforest object}
\end{figure}



% baggtree this function grow a PPtreeclass using PPtree\_split for each bootstrap sample.
% This function returns a data frame with the results from PPtree\_split for each bootsrap samples.
% m is the number of trees here is a small example.
% <<>>=
%  crab.trees <- baggtree(data = crab, class = "Type",
%   m =  10, PPmethod = 'LDA', size.p = 0.6 )
%  str(crab.trees, max.level = 1)
% 
%  #selecting first PPtree object,
%  crab.trees[[1]][[1]]
% @

% 
% The PPforest algorithm calculates variable importance in two ways: (1) permuted importance using accuracy,  and (2) importance based on projection coefficients on standardized variables.
% 
% The permuted variable importance is comparable with the measure defined in the classical random forest algorithm. It is computed using the out of bag (oob) sample for the tree $k\;\;(B^{(k)})$ for each $X_j$ predictor variable.  Then the
% permuted importance of the variable $X_j$ in the tree $k$ can be defined as:
% 
% \[
% IMP^{(k)}(X_j) = \frac{\sum_{i \in B^{(k)} } I(y_i=\hat y_i^{(k)})-I(y_i=\hat y_{i,P_j}^{(k)})}{|B^{(k)}|}
% \]
% 
%  where $\hat y_i^{(k)}$
% is the predicted class for the observation $i$ in the tree $k$ and $y_{i,P_j}^{(k)}$ is the predicted class for the observation $i$ in the tree $k$ after permuting the values for variable $X_j$. The global permuted importance measure is the average importance over all the trees in the forest.
% This measure is based on comparing the accuracy of classifying out-of-bag observations, using the true class with permuted (nonsense) class.
% To compute this measure you should use permute\_importance function.
% 
% \begin{figure}
% <<fig.show='hold', fig.width = 5, fig.height = 5, echo=FALSE>>=
% impo1 <- permute_importance(pprf.crab) 
% impo1 
% 
% ggplot(impo1, aes(x = imp, y = nm)) + geom_point() 
% 
%   @
%   \end{figure}
%  This function returns a data frame with permuted importance measures, imp is the permuted importance measure defined in Brieman paper, imp2 is the permuted importance measure defined in randomForest package, the standard deviation (sd.im and sd.imp2) for each measure is computed and the also the standardized measure. 
% 
% For the second importance measure, the coefficients of each projection are examined. The magnitude of these values indicates importance, if the variables have been standardized. The variable importance for a single tree is computed by a weighted sum of the absolute values of the coefficients across nodes. The weights takes the number of classes in each node into account~\citep{lee2013pptree}. 
% Then the importance of the variable $X_j$ in the PPtree $k$ can be defined as: 
%  \[ 
%  IMP_{pptree}^{(k)}(X_j)=\sum_{nd = 1}^{nn}\frac{|\alpha_{nd}^{(k)}|}{cl_{nd} } 
%  \] 
%  Where $\alpha_{nd}^{(k)}$ is the projected coefficient for node $ns$ and variable $k$ and $nn$ the total number of node partitions in the tree $k$.
% 
% The global variable importance in a PPforest then can be defined in different ways. The most intuitive is the average variable importance from each PPtree across all the trees in the forest.
%  \[
%  IMP_{ppforest1}(X_j)=\frac{\sum_{k=1}^K IMP_{pptree}^{(k)}(X_j)}{K} 
% \] 
%  Alternatively we have defined a global importance measure for the forest as a weighted mean of the absolute value of the projection coefficients across all nodes in every tree. The weights are based on the projection pursuit indexes in each node ($Ix_{nd}$), and 1-(OOB-error of each tree)($acc_k$).
% 
% \[IMP_{ppforest2}(X_j)=\frac{\sum_{k=1}^K acc_k \sum_{nd = 1}^{nn}\frac{Ix_{nd}|\alpha_{nd}^{(k)}|}{nn }}{K}
% \]
% 
% <<>>=
% impo2 <-  ppf_avg_pptree_imp(pprf.crab, "Type")
% impo2 
% ggplot(impo2, aes(x = mean, y = variable) ) +geom_point() 
% @
%  Finally you can get the last importance measure we have proposed for the PPforest using ppf\_global\_imp function.
% \begin{figure}
% <<fig.show='hold', fig.width = 5, fig.height = 5, echo=FALSE>>=
% impo3 <- ppf_global_imp(data = crab, class = "Type", pprf.crab)  
% impo3
% ggplot(impo3, aes(x = mean, y = variable) ) + geom_point() 
% @
% \end{figure}
% If we compare the results with the `randomForest` function for this data set the results are the following: 
% 
% <<>>=
% rf.crab <- randomForest::randomForest(Type~., data = crab, proximity = TRUE, ntree = 100) 
% rf.crab
% @
% We can see that for this data set the `PPforest` performance is much better than using \pkg{randomForest}. \pkg{PPforest} works well since the classes can be separated by linear combinations of variables. 
% This is a clear case where oblique hyperplanes are more adequate in this case than hyperplanes horizontal to the axis. 
% 
%  Using the information available in the PPforest object, some visualization can be done. I will include some useful examples to visualize the data and some of the most important diagnostics in a forest structure. 
%  
% To describe the data structure a parallel plot can be done, the data were stundarized and the color represents the class variable.
% \begin{figure}
% <<fig.show='hold',fig.width = 5 ,fig.height = 4, warning=FALSE, echo=FALSE>>=
%  
% parallel <- function(ppf){ 
% myscale <- function(x) (x - mean(x)) / sd(x) 
% 
% scale.dat <- ppf$train %>% dplyr::mutate_each(dplyr::funs(myscale),-dplyr::matches(ppf$class.var)) 
% scale.dat.melt <- scale.dat %>%  dplyr::mutate(ids = 1:nrow(ppf$train)) %>% tidyr::gather(var,Value,-Type,-ids) 
%  scale.dat.melt$Variables <- as.numeric(as.factor(scale.dat.melt$var)) 
%  colnames(scale.dat.melt)[1] <- "Class" 
%  ggplot2::ggplot(scale.dat.melt, ggplot2::aes(x = Variables, y = Value, 
%                           group = ids, key = ids, colour = Class, var = var)) +
%  ggplot2::geom_line(alpha = 0.3) + ggplot2::scale_x_discrete(limits = levels(as.factor(scale.dat.melt$var)), expand = c(0.01,0.01)) + 
%   ggplot2::ggtitle("Data parallel plot ") + ggplot2::theme(legend.position = "none", axis.text.x  = element_text(angle = 90, vjust = 0.5)) + 
%   ggplot2::scale_colour_brewer(type = "qual", palette = "Dark2") 
% }
% parallel(pprf.crab)
% @
% \end{figure}
% Some  auxiliary functions are available in `PPforest` to get the data structure needed to do some visualization.  Because the PPforest is composed of many tree fits on subsets of the data, a lot of statistics can be calculated to analyze as a separate data set, and better understand how the model is working. -->
% Some of the diagnostics of interest are: variable importance, OOB error rate, vote matrix and proximity matrix. Also will be possible to explore the individual model level. 
% 
% With a decision tree we can compute for every pair of observations the proximity matrix. This is a $nxn$ matrix where if two cases $k_i$ and $k_j$ are in the same terminal node increase their proximity by one, at the end normalize the proximities by dividing by the number of trees
% To visualize the proximity matrix we use a scatter plot with information from multidimensional scaling method. In this plot color indicates the true species and sex. For this data two dimensions are enough to see the four groups separated quite well. Some crabs are clearly more similar to a different group, though, especially in examining the sex differences. 
%  
%  \begin{figure}
% <<fig.show='hold',fig.width = 5 ,fig.height = 4, warning=FALSE, echo=FALSE>>=
% mdspl2d <- function(ppf, lege = "bottom", siz = 3, k = 2) { 
%  d <- diag(nrow(ppf$train))
%  d <- as.dist(d + 1 - ppf$proximity) 
%  rf.mds <- stats::cmdscale(d, eig = TRUE,  k = k) 
%  colnames(rf.mds$points) <- paste("MDS", 1:k, sep = "")
%  df <- data.frame(Class = ppf$train[, 1], rf.mds$points) 
%   mds <- ggplot2::ggplot(data = df) + 
%   ggplot2::geom_point(ggplot2::aes(x = MDS1, y = MDS2, color = Class), 
%             size = I(siz), alpha = .5) + 
%  ggplot2::scale_colour_brewer(type = "qual", palette = "Dark2", name = "Class") + 
%  ggplot2::theme(legend.position = lege, aspect.ratio = 1) 
%   mds 
% } 
% 
%  mdspl2d(pprf.crab) 
%  @
%  \end{figure}
%  The vote matrix ($n \times p$) contains the proportion of times each observation was classified to each class, whole oob. Two possible approaches to visualize the vote matrix information are shown, with a side-by-side jittered dot plot or with ternary plots. 
%  A side-by-side jittered dotplot is used for the display, where class is displayed on one axis and proportion is displayed on the other. For each dotplot, the ideal arrangement is that points of observations in that class have values bigger than 0.5, and all other observations have less. This data is close to the ideal but not perfect, e.g. there are a few blue male crabs (orange) that are frequently predicted to be blue females (green), and a few blue female crabs predicted to be another class. 
% \begin{figure}
%  <<fig.show='hold',fig.width = 5 ,fig.height = 5, warning = FALSE, echo=FALSE>>=
%  side <-  function(ppf, ang = 0, lege = "bottom", siz = 3,
%                    ttl = "Side by side dotplot") { 
%  voteinf <- data.frame(ids = 1:length(ppf$train[, 1]), Type = ppf$train[, 1],
%                       ppf$votes, pred = ppf$prediction.oob ) %>% 
%   tidyr::gather(Class, Probability, -pred, -ids, -Type) 
%    ggplot2::ggplot(data = voteinf, ggplot2::aes(Class, Probability, color = Type)) + 
%  ggplot2::geom_jitter(height = 0, size = I(siz), alpha = .5) +
%     ggtitle(ttl) + 
%     ylab("Proportion") + 
%     ggplot2::scale_colour_brewer(type = "qual", palette = "Dark2") + 
%    ggplot2::theme(legend.position = lege, legend.text = ggplot2::element_text(angle = ang)) + 
%    ggplot2::labs(colour = "Class")
%  }
%  
%  side(pprf.crab) 
% @
% \end{figure}
%  A ternary plot is a triangular diagram that shows the proportion of three variables that sum to a constant and is done using barycentric coordinates. Compositional data lies in a $(p-1)$-D simplex in $p$-space. 
%  One advantage of ternary plot is that are good to visualize compositional data and the proportion of three variables in a two dimensional space can be shown. 
%  When we have tree classes a ternary plot are well defined. With more than tree classes the ternary plot idea need to be generalized. \cite{sutherland2000orca} suggest the best approach to visualize compositional data will be to project the data into the $(p-1)-$D space (ternary diagram in $2-D$)  This will be the approach used to visualize the vote matrix information.  
%  A ternary plot is a triangular diagram used to display compositional data with three components. More generally, compositional data can have any number of components, say $p$, and hence is contrained to a $(p-1)$-D simplex in $p$-space. The vote matrix is an example of compositional data, with $G$ components. 
% 
% \begin{figure}
% <<fig.show='hold',fig.width = 7 ,fig.height = 4, warning=FALSE, echo=FALSE>>=
%  pl_ter <- function(dat, dx, dy ){ 
%  p1  <- dat[[1]] %>% dplyr::filter(pair %in% paste(dx, dy, sep = "-") ) %>% 
%   dplyr::select(Class, x, y) %>% 
%  ggplot2::ggplot(aes(x, y, color = Class)) + 
%  ggplot2::geom_segment(data = dat[[2]], aes(x = x1, xend = x2, 
%                                 y = y1, yend = y2), color = "black" ) + 
%   ggplot2::geom_point(size = I(3), alpha = .5) + 
%    ggplot2::labs(y = " ",  x = " ") + 
%    ggplot2::theme(legend.position = "none", aspect.ratio = 1) + 
%  ggplot2::scale_colour_brewer(type = "qual", palette = "Dark2") + 
%    ggplot2::labs(x = paste0("T", dx, ""), y = paste0("T", dy, " ")) + 
%   ggplot2::theme(aspect.ratio = 1) 
%  p1 
%  }
%   p1 <-  pl_ter(ternary_str(pprf.crab, id = c(1, 2, 3), sp = 3, dx = 1, dy = 2), 1, 2 ) 
%  p2 <-  pl_ter(ternary_str(pprf.crab, id = c(1, 2, 3), sp = 3, dx = 1, dy = 3), 1, 3) 
%  p3 <-  pl_ter(ternary_str(pprf.crab, id = c(1, 2, 3), sp = 3, dx = 2, dy = 3), 2, 3) 
%  
%  gridExtra::grid.arrange(p1, p2, p3, ncol = 3)
% @
% \end{figure}
% 
%  To see a complete description about how to visualize a PPforest object read Interactive Graphics for Visually Diagnosing Forest Classifiers in R (Include reference). 
% \begin{figure}
%  << fig.show = 'hold',fig.width = 5 ,fig.height = 4, warning = FALSE>>=
%  impotree <- function(data,ppf, nodes = c(1, 2, 3), tree ){ 
%  bnf <- function(x) { 
%    bn <- abs(x$projbest.node) 
%   bn[bn == 0] <- NA 
%    data.frame(node = 1:dim(x$projbest.node)[1], bn)
%  } 
% 
%  bestnode <- ppf[["output.trees"]] %>%  lapply(bnf) %>% dplyr::bind_rows() 
%  colnames(bestnode)[-1] <- colnames(data[, -1]) 
%  bestnode$node <- as.factor(bestnode$node)
%  aux <- bestnode %>% dplyr::filter(node %in% nodes) %>% dplyr::mutate(ids = rep(1:ppf$n.tree, each = length(nodes))) %>% tidyr::gather(var, value, -ids, -node)
% aux$Variables <- as.numeric(as.factor(aux$var) ) 
%  aux$Abs.importance <- round( aux$value, 2) 
%  p <- ggplot2::ggplot(dplyr::filter(aux, !ids %in% tree), 
%      ggplot2::aes(x = Variables , y = Abs.importance , group = ids)) +    ggplot2::geom_jitter(height = 0, size = I(3), alpha = 0.3) + 
%   ggplot2::facet_grid(node ~ .) + ggplot2::scale_x_discrete(limits = levels(as.factor(aux$var)) ) + ggplot2::theme(legend.position = "none") 
%     p + ggplot2::geom_jitter(
%    data = dplyr::filter(aux, ids %in% tree), 
%    ggplot2::aes( 
%     x = Variables , 
%     y = Abs.importance ,
%     group = ids, 
%     colour = "red" 
%   ),
%    height = 0, 
%    size = I(3) 
%  ) + ggplot2::facet_grid(node ~ .)  + 
%   ggplot2::theme(legend.position = "none") + ggplot2::labs(y = "Absolute coefficient") 
%  } 
%  impotree(crab, pprf.crab, tree =1 ) 
% @
%\end{figure}

%  From PPforest object we can plot a heat map of the proximity matrix using the function pproxy\_plot. 
% This function has tree arguments, ppf is a PPforest object, type an argument that specify if the plot is a heatmap or a MDS plot.
% If the plot is a MDS plot the argument k defines the number of MDS layouts. 
%  This function return an interactive plot based on plotly` package if `interactive = TRUE`. -->
% 
% 
% <!-- ```{r,fig.show='hold',fig.width = 5 ,fig.height = 4, warning=FALSE} -->
% 
% <!-- PPforest::pproxy_plot(pprf.crab, type = "heat", interactive = FALSE) -->
% 
% <!-- ``` -->
% 
% <!-- In this plot we can see a heat map for the proximity matrix, we can observe that strong red color indicates that the observations are more similar. -->
% <!-- The data are ordered by class (BlueFemale, BlueMale, OrangeFemale and OrangeMale), in the heat map we can observe a colored block diagonal structure, this means that the observations from the same class are similar here the same class were classified most of the time in the correct class, but also we can observe that observations from BlueMale and BlueFemale are similar too then some data were classified in the incorrect class. -->
% 
% 
% <!-- Additionally `pproxy_plot` can be used to plot the MDS using proximity matrix information. -->
% 
% <!-- If we select k =2 the output plot is as follows: -->
% 
% <!-- ```{r,fig.show='hold',fig.width = 6 ,fig.height = 4, warning = FALSE} -->
% <!-- PPforest::pproxy_plot(pprf.crab, type="MDS", k =2, interactive = FALSE ) -->
% <!-- ``` -->
% 
% 
% <!-- We can observe a spatial separation between classes. Orange (male and female) are more separated than Blue (male and female). -->
% 
% <!-- If we select k>2,  we can observe that using two dimensions is enough to see the spatial separation. -->
% 
% 
% <!-- ```{r,fig.show='hold',fig.width = 6 ,fig.height = 6, warning=FALSE} -->
% <!-- PPforest::pproxy_plot(pprf.crab, type="MDS",k = 3, interactive = TRUE) -->
% <!-- ``` -->
% 
% <!-- Another possible visualization in `PPforest` package is for the importance measure. -->
% 
% <!-- The variable importance for the group separation can be measured by the projection coefficients in each individual tree. Based on these coefficients we can examine how the classes are separated and which variables are more relevant for the separation. -->
% 
% <!--  In `PPtree` the projection coefficient of each node represent the importance of variables to class separation in each node. Since in `PPforest` we have `m` trees we can define a global importance measure.  For this global importance measure we need to take into account the importance in each node and combine the results for all the trees in the forest. The importance measure of `PPforest` is a  weighted mean of the absolute value of the projection coefficients across all nodes in every tree. The weights are  the projection pursuit index in each node, and 1-the out of bag error of each tree. -->
% 
% <!-- `ppf_importance` has four arguments, data (data frame with the complete data set), class (a character with the name of the class variable), global ( logical that indicate is the importance measure is global or not) and weight (logical argument that indicates if the importance measure is weighted or not). -->
% 
% 
% <!-- Using the `ppf_importance` function we can plot the weighted global importance measure in `PPforest`. -->
% 
% <!-- ```{r, fig.show='hold',fig.width = 5 ,fig.height = 4, warning=FALSE} -->
% <!-- PPforest::ppf_importance(data = crab, class = "Type", pprf.crab, global = TRUE, weight = TRUE, interactive = FALSE) -->
% <!-- ``` -->
% 
% <!-- We can see that the most important variable in this example is RW while the less important is BD. -->
% <!-- An importance measure for each node also is available. -->
% 
% 
% <!-- ```{r, fig.show='hold',fig.width = 5 ,fig.height = 4, warning=FALSE} -->
% <!-- PPforest::ppf_importance(data = crab, class = "Type", pprf.crab, global = FALSE, weight = TRUE, interactive = FALSE) -->
% <!-- ``` -->
% 
% <!-- The importance variable order is the same for node 1 and node 2 but different in node 3. -->
% 
% <!-- Finally a cumulative out of error plot can be done using `ppf_oob_error` -->
% <!-- This function has three arguments, ppf (`PPforest` object), nsplit1 (number, increment of the sequence where cumulative oob error rate is computed in the  1/3 trees) and interactive if we want a interactive plot. -->
% 
% 
% <!-- ```{r, fig.show='hold',fig.width = 6 ,fig.height = 4, warning=FALSE} -->
% <!--  PPforest::ppf_oob_error(pprf.crab, nsplit1 = 15, interactive = FALSE) -->
% <!-- ``` -->
% 
% <!-- The oob-error rate decrease when we increase the numbers of trees but gets constant with less than 70 trees. -->
% 
% 
% 
% <!--        a <- ggplot2::ggplot(import.vi.wg, ggplot2::aes(x = mean, y = variable)) + ggplot2::geom_point() + ggplot2::theme(aspect.ratio=1) -->
% <!--        print(import.vi.wg) -->
% 
% <!--      plotly::ggplotly(a) -->



